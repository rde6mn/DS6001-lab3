{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 5: Web Scraping\n",
    "## DS 6001\n",
    "\n",
    "### Instructions\n",
    "Please answer the following questions as completely as possible using text, code, and the results of code as needed. Format your answers in a Jupyter notebook. To receive full credit, make sure you address every part of the problem, and make sure your document is formatted in a clean and professional way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 1\n",
    "Large language models are very impressive. They use deep neural networks with billions of parameters. They fine-tune results with clever approaches to reinforcement learning through human feedback. Many of them employ APIs with user interfaces that allow users to chat in natural language through a simple textbox, and receive a response in only a few seconds. And the societal impact of these models is undeniably enormous, to an extent we are only beginning to understand.\n",
    "\n",
    "But, none of that is the most impressive part of LLMs. Like anything else in machine learning, the most impressive and difficult element of the work is the data collection.\n",
    "\n",
    "The data used to train major LLMs is something that big companies communicate very little about. [OpenAI](https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-foundation-models-are-developed) only says that GPT and other baseline models are trained on data that is \"freely and openly accessible on the internet.\" Many commentators, such as Dr. Vered Shwartz, assistant professor in the University of British Columbia department of computer science, say that LLMs like GPT are trained on \"[essentially the entire internet](https://science.ubc.ca/news/chatgpt-has-read-almost-whole-internet-hasnt-solved-its-diversity-issues).\" Think about pulling the entire internet into a single model and tell me this isn't the most impressive thing about ChatGPT.\n",
    "\n",
    "A primary training set for Open AI's GPT and other major LLMs is the corpus compiled by [Common Crawl](https://commoncrawl.org/faq), a nonprofit organization that describes itself as \"dedicated to providing a copy of the Internet to Internet researchers, companies and individuals at no cost for the purpose of research and analysis.\" According to the [Mozilla Foundation](https://www.mozillafoundation.org/en/blog/Mozilla-Report-How-Common-Crawl-Data-Infrastructure-Shaped-the-Battle-Royale-over-Generative-AI/), \"[o]ver 80% of GPT-3 tokens (a representation unit of text data) stemmed from Common Crawl. Many models published by other developers likewise rely heavily on it: the study analyzed 47 LLMs published between 2019 and October 2023 that power text generators and found at least 64% of them were trained on Common Crawl.\"\n",
    "\n",
    "Common Crawl is a massive web scraping endeavor. But it's not necessarily a sophisticated one. Mostly, Common Crawl is downloading the raw HTML from the webpages it scrapes and extracting text from the HTML. A task like this is exactly the kind of thing we can use `requests` and `BeautifulSoup` to do. \n",
    "\n",
    "Common Crawl is also at the center of many controversies and legal actions regarding generative AI, such as the New York Times' copyright infringement [lawsuit](https://www.npr.org/2025/03/26/nx-s1-5288157/new-york-times-openai-copyright-case-goes-forward) against OpenAI, and concerns about bias and racism in LLMs stemming from their training data.\n",
    "\n",
    "For this problem, please examine the [Common Crawl website](https://commoncrawl.org/) and read \"[Training Data for the Price of a Sandwich: Common Crawl’s Impact on Generative AI](https://assets.mofoprod.net/network/documents/Common_Crawl_Mozilla_Foundation_2024.pdf)\" by Stefan Baack for the Mozilla Foundation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "If you want access to the latest Common Crawl dataset, where can you get it? How many website does the dataset contain? And outside of computational and storage costs, how much will it cost to obtain the data? (Use [Common Crawl's website](https://commoncrawl.org/) to answer this question) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Is Common Crawl more useful for the pre-training or fine-tuning stage of the development of an LLM? Why is the Common Crawl corpus used by so many AI efforts, and why must it usually be altered or filtered in some way? (see pages 11-13 of Baack's article) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Given how Common Crawl decides which URLs to crawl, what are three reasons why the data cannot be said to be the complete internet or a representative sample of it? (see pages 17-22 of Baack's article) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Of the suggestions that the Mozilla Foundation make for the future development of Common Crawl, are there any that you especially agree with or disagree with, and why? (See pages 30-31 of Baack's article) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 2\n",
    "For the following problems, you will be scraping http://books.toscrape.com/. This website is a fake book retailer, designed to mimic the design of many retail websites. It exists solely to help students practice web-scraping, so there aren’t going to be any ethical concerns with this particular exercise, and there shouldn’t be any issues with rate limits or other gates that could prevent web-scraping. Take a moment and look at this website, so that you know what you will be working with.\n",
    "\n",
    "Your goal is to generate a dataframe with four columns: one for the title, one for the price, one for the star-rating, and one or the book cover JPEG’s URL. The dataframe will also 1000 rows, one for each of the 1000 books listed on the 50 pages of this website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "Pull the HTML code from http://books.toscrape.com/. Make sure you provide the correct user agent string. Then parse this HTML code and save the parsed code as a separate Python variable. [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Extract all 20 of the book titles and save them in a list. [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Extract the price of each of the 20 books and save these prices in a list. (The prices are listed in British pounds, and include the £ symbol. Remove the £ symbols: if you’ve saved the prices in a list named `prices`, then the following code should work: `prices = [s.replace('Â£', '') for s in prices]`.) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Extract the star level ratings for the 20 books. [Hint: for tags such as `<p class=\"star-rating One\">` in which the class has a space, the class is actually a list in which the first item in the list is `\"star-rating\"` and the second item in the list is `\"One\"`. It's possible to search on either item in this list.][8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e\n",
    "Extract the URLs for the JPEG thumbnail images that show the covers of the 20 books. (Maybe we want to mine the images to build models that predict the star level, literally judging books by their covers.) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part f\n",
    "Create a dataframe with one row for each of the 20 books, and the book titles, prices, star ratings, and cover JPEG URLs as the four columns. [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part g\n",
    "Create a function that takes the URL of the webpage to scrape as an input, applies the code you wrote for parts a through e, and generates the dataframe from part f as the output. [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part h\n",
    "Notice that there are many pages to http://books.toscrape.com/. When you click on “Next” in the bottom-right corner of the screen, it takes you to http://books.toscrape.com/catalogue/page-2.html. The front page is the same as http://books.toscrape.com/catalogue/page-1.html, and there are 50 total pages.\n",
    "\n",
    "Write a loop that uses the function you wrote in part g to scrape each of the 50 pages, and append each of these data frames together. If you write this loop correctly, your dataframe will have 1000 rows (20 books on each of the 50 pages). \n",
    "\n",
    "Some hints:\n",
    "\n",
    "* Typing `new_df = pd.DataFrame()` with nothing in the parentheses will create an empty data frame on which new data can be appended.\n",
    "\n",
    "* There are many loops you can use, but the most straightforward one is a for-values loop that counts from 1 to 50. In Python, you can initialize such a loop with for i in range(1, 51):, and indenting every line below it that belongs inside the loop. Inside the loop, the letter i is now a stand-in for the number currently being considered.\n",
    "\n",
    "* You will need to figure out how to replace the number in URLs like http://books.toscrape.com/catalogue/page-2.html with the number currently under consideration in the loop. You might need the `str()` function, which turns numeric values into strings.\n",
    "\n",
    "* `pd.concat()` is a method that appends dataframes together.\n",
    "\n",
    "[10 points]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds6001summer2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
